
ex: 

099396f3224433e51d27	why do we choose to tolerate things rather than follow our own feelings 	0
21aeec3f8e01c60ba22b	what do you think about the giant petrified trees conspiracy 	0



===============================================================================================================================

# print('Tokenize texts...')


train_dataset.tokens[0] 


['why', 'do', 'we', 'choose', 'to', 'tolerate', 'things', 'rather', 'than', 'follow', 'our', 'own', 'feelings']


train_dataset.tokens[1] 

['what', 'do', 'you', 'think', 'about', 'the', 'giant', 'petrified', 'trees', 'conspiracy']


print(train_dataset.tokens)

[list(['why', 'do', 'we', 'choose', 'to', 'tolerate', 'things', 'rather', 'than', 'follow', 'our', 'own', 'feelings']), list(['what', 'do', 'you', 'think', 'about', 'the', 'giant', 'petrified', 'trees', 'conspiracy'])]



print(submit_dataset.tokens)

[list(['why', 'do', 'zainichi', 'koreans', '在日朝鮮人／韓国人', 'dislike', 'anton', 'antenorcruz']), list(['does', 'delhi', 'university', 'ask', 'questions', 'from', 'general', 'aptitude', 'subjects', 'like', 'general', 'science', 'maths', 'english', 'in', 'the', 



[array([list(['why', 'do', 'we', 'choose', 'to', 'tolerate', 'things', 'rather', 'than', 'follow', 'our', 'own', 'feelings']),
       list(['what', 'do', 'you', 'think', 'about', 'the', 'giant', 'petrified', 'trees', 'conspiracy'])],
      dtype=object), array([], dtype=object), array([list(['why', 'do', 'zainichi', 'koreans', '在日朝鮮人／韓国人', 'dislike', 'anton', 'antenorcruz']),
       list(['does', 'delhi', 'university', 'ask', 'questions', 'from', 'general', 'aptitude', 'subjects', 'like', 'general', 'science', 'maths', 'english', 'in', 'the', 'm', 'a', 'entrance', 'examination'])],
      dtype=object)]


===============================================================================================================================

print('Build vocabulary...')
vocab = preprocessor.build_vocab(datasets, config)

vocab

Counter({'do': 3, 'the': 2, 'why': 2, 'giant': 1, 'subjects': 1, 'choose': 1, 'than': 1, 'zainichi': 1, 'anton': 1, 'we': 1, 'examination': 1, 'questions': 1, 'dislike': 1, 'university': 1, 'rather': 1, 'in': 1, 'tolerate': 1, 'a': 1, 'does': 1, 'feelings': 1, 'our': 1, 'conspiracy': 1, 'what': 1, 'ask': 1, 'own': 1, '在日朝鮮人／韓国人': 1, 'you': 1, 'to': 1, 'm': 1, 'koreans': 1, 'maths': 1, 'general': 1, 'about': 1, 'trees': 1, 'think': 1, 'follow': 1, 'from': 1, 'antenorcruz': 1, 'things': 1, 'like': 1, 'delhi': 1, 'science': 1, 'english': 1, 'aptitude': 1, 'entrance': 1, 'petrified': 1})


self.word_freq

{'giant': 1, 'subjects': 1, 'choose': 1, 'than': 1, 'zainichi': 1, 'anton': 1, 'we': 1, 'examination': 1, 'questions': 1, 'university': 1, 'rather': 1, 'in': 1, 'tolerate': 1, 'delhi': 1, 'does': 1, 'feelings': 1, 'science': 1, 'conspiracy': 1, 'what': 1, 'ask': 1, 'own': 1, 'follow': 1, '在日朝鮮人／韓国人': 1, 'you': 1, 'english': 1, 'the': 2, 'to': 1, 'm': 1, 'koreans': 1, 'maths': 1, 'general': 1, 'about': 1, 'trees': 1, 'think': 1, 'dislike': 1, 'antenorcruz': 1, 'from': 1, 'why': 2, 'things': 1, 'like': 1, 'a': 1, 'our': 1, 'do': 3, '<PAD>': 0, 'aptitude': 1, 'entrance': 1, 'petrified': 1}


self.token2id

{'giant': 1, 'dislike': 10, 'choose': 3, 'than': 4, 'zainichi': 5, 'anton': 6, 'we': 7, 'examination': 8, 'questions': 9, 'university': 11, 'rather': 12, 'in': 13, 'tolerate': 14, 'delhi': 40, 'does': 16, 'feelings': 17, 'science': 41, 'conspiracy': 19, 'what': 20, 'ask': 21, 'own': 22, 'follow': 34, '在日朝鮮人／韓国人': 23, 'you': 24, 'english': 43, 'the': 25, 'to': 26, 'm': 27, 'koreans': 28, 'maths': 29, 'general': 30, 'about': 31, 'trees': 32, 'think': 33, 'subjects': 2, 'antenorcruz': 36, 'from': 35, 'why': 37, 'entrance': 38, 'like': 39, 'a': 15, 'our': 18, 'do': 42, '<PAD>': 0, 'aptitude': 44, 'things': 45, 'petrified': 46}


self.lfq 

its basically check with condition less then mincount value (its 5) based on that prepare boolean value 

self.hfq

its basically return false value greaterthen value mincount with boolean value 

================================================================================================
================================================

********** print('Build token ids...') it's basically arrange "sentance with token-id"

print(tokenids) it's return list 3 array (train_dataset.tids , test_dataset.tids, submit_dataset.tids)

in config maxlen - 72

train_dataset.tids define shape is (2,72) .. variable assign value into "tids"


[array([[37, 42,  7,  3, 26, 14, 45, 12,  4, 34, 18, 22, 17,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0],
       [20, 42, 24, 33, 31, 25,  1, 46, 32, 19,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32), array([], shape=(0, 72), dtype=int32), array([[37, 42,  5, 28, 23, 10,  6, 36,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0],
       [16, 40, 11, 21,  9, 35, 30, 44,  2, 39, 30, 41, 29, 43, 13, 25,
        27, 15, 38,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)]


================================================================================================
================================================

ApplyNdArray accept function same as return function  .

======================================================================



*************** print('Build sentence extra features...')

n_dims = 0 


*******************  preprocessor.build_sentence_features


train_dataset._X2 it's basically retun [] - ndarray(2, 0)


********[d.build(config.device) for d in datasets]

in def build(self, device)

access self.tids (basically its ndarray type) and convert into tensor ...same to work on df.target and df.weight variable also convert type into tensor  .

self._t, self._W , self.t, self.W


self.X tensor
============


tensor([[37, 42,  7,  3, 26, 14, 45, 12,  4, 34, 18, 22, 17,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],
        [20, 42, 24, 33, 31, 25,  1, 46, 32, 19,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])


tensor([[37, 42,  5, 28, 23, 10,  6, 36,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],
        [16, 40, 11, 21,  9, 35, 30, 44,  2, 39, 30, 41, 29, 43, 13, 25, 27, 15,
         38,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])


===================================================================================================


****************print('Load pretrained vectors...')

load pretrained vectors pass ::: 1. config of pretrained vectors 2. vocab.token2id 3. 

ex:: 


config of pretrained vectors :: glove


vocab.token2id

{'giant': 1, 'dislike': 10, 'choose': 3, 'than': 4, 'zainichi': 5, 'anton': 6, 'we': 7, 'examination': 8, 'questions': 9, 'university': 11, 'rather': 12, 'in': 13, 'tolerate': 14, 'delhi': 40, 'does': 16, 'feelings': 17, 'science': 41, 'conspiracy': 19, 'what': 20, 'ask': 21, 'own': 22, 'follow': 34, '在日朝鮮人／韓国人': 23, 'you': 24, 'english': 43, 'the': 25, 'to': 26, 'm': 27, 'koreans': 28, 'maths': 29, 'general': 30, 'about': 31, 'trees': 32, 'think': 33, 'subjects': 2, 'antenorcruz': 36, 'from': 35, 'why': 37, 'entrance': 38, 'like': 39, 'a': 15, 'our': 18, 'do': 42, '<PAD>': 0, 'aptitude': 44, 'things': 45, 'petrified': 46}



freqs = np.zeros((len(token2id)), dtype='f')  (47 array build)

ex ::: [0.0.0.0............] (47 array build)

len(token2id) :: 47 (our total Build vocabulary)

embed_shape : (47, 300) 

vectors = np.zeros(embed_shape, dtype='f')  (47 rows , 300 column)


while iterating one by one our golve embedding and check with exitsing our vocab / tokenids if match then set 
"1" in freqs array 

example ::  freqs[token2id[token]] += 1


while iterating one by one our golve embedding and check with exitsing our vocab / tokenids if match then set 
full vector in vectors(same tokenids ex:::: token2id[token] )


example :::

o = "the 1 2 3 4 5 ...300" ----- is len(301)

token, *vector = o.split(' ')

token value is :: the

vector value is :: [1,2,3,4,5....300] that formar is basically list so need to convert into numpy array for shape 

ex .. :  np.array(vector, 'f')
we can check np.array(vector, 'f').shape is  (300)

then assign vector to vectors 

vectors[token2id[token]] += np.array(vector, 'f')

- vectors[freqs != 0] /= freqs[freqs != 0][:, None] 

select value from numpy array (freqs) based on condition is not equals 0  . than change shape example 

freqs[freqs != 0][:, None] - same as "numpy.vstack(freqs[freqs != 0])" 

#one = vectors[freqs != 0]
vectors[freqs != 0] /= freqs[freqs != 0][:, None]
#two = vectors[freqs != 0]
#result = one == two
#print("after::::", result.all())  # so basically we can see there is no any change is there between one to two vector  


- dividing-numpy-array-by-numpy-array , dividing-numpy-array-by-numpy-scalar
a = np.array([[1.0,2.0],[3.9,4.9]])
b = np.array([1.0,1.0])
b=np.vstack(b)
a /= b
a



after vectors ready we can put into words with vecotors into Word2VecKeyedVectors class

#KeyedVectors gensim - models keyvectors

class Word2VecKeyedVectors :::::::
 |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.
 |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.


vec = KeyedVectors(300)
add(entities, weights, replace=False) method of gensim.models.keyedvectors.Word2VecKeyedVectors instance


Parameters
----------
entities : list of str
    Entities specified by string ids.
weights: {list of numpy.ndarray, numpy.ndarray}
    List of 1D np.array vectors or a 2D np.array of vectors.
replace: bool, optional
    Flag indicating whether to replace vectors for entities which already exist in the vocabulary,
    if True - replace vectors, otherwise - keep old vectors.


return value is :::: vocab.token2id [rows] * 300 [columsn] 


*******************print('Build word embedding matrix...')


input is ::: WordEmbeddingFeaturizer(config, vocab)

1. vocab :: Build vocabulary 
2. config parameter

return output is :::: word_embedding_featurizer
1.cofig 2.default-config 3.extra_default_config 4.vocab 5.featurizers 6. registry


input is ::: preprocessor.build_embedding_matrices(
        datasets, word_embedding_featurizer, vocab, pretrained_vectors)

 
 return output is ::: we have 2 ndarray ..1. pretrained 2. word2vec 



question ::: how to call Any2VecFeaturizer class insteadof @word2vec / Word2VecFeaturizer class..?
ans ::  basically its simple inhertance is there and in word2vecfeaturizer class is not there init method so An2vecfeaturizer class contains init method so class of An2vecfeaturizer init method is directly call .


in build_embedding_matrices method first we get vectors from "pretrained_vectors.values()" assign value to "test" list varaible .


example 
import numpy as np
a1 = np.array([1,2,3])
a2 = np.array([4,5,6])
np.stack(a1,a2)
output :: 

array([[1, 2, 3],
       [4, 5, 6]])

np.stack((a1,a2)).mean(axis=0)
output :: array([2.5, 3.5, 4.5])


a3 = np.array([7,8,0])
r1 = np.stack((a1,a2,a3))

(r1 == 0).all(axis=0)
(r1 == 0).all(axis=1)


false to true ==> true to false
ex::

ac = np.array([True,False,True])
~ac
array([False,  True, False])
it's is basically tilde sign = ~(r1==0).all(axis=1)


== embedding_matrices = word_embedding_featurizer(pretrained_vectors_merged, datasets)

__call__ method ::: we are used basic call method to call different instances with pass value 

- class Example:
    def __init__(self):
        print("Instance Created")

    # Defining __call__ method
    def __call__(self):
        print("Instance is called via special method")

# Instance created
e = Example()

# __call__ method will be called
e()


# __call__ method of class WordEmbeddingFeaturizerWrapper in - featurizer.py
#features = pretrained_vectors_merged
#featurizers = 1
# 1 . ('pretrained', <qiqc.preprocessing.modules.featurizers.word_embedding_features.PretrainedVectorFeaturizer object)
# 2. ('word2vec', <qiqc.preprocessing.modules.featurizers.word_embedding_features.Word2VecFeaturizer object )


== feat-> self.featurizers.items() ==>  word_embedding_features.Word2VecFeaturizer ==> __call__ method


== initialW = features.copy()

import numpy
a = numpy.array([[0,1,2,3],[4,5,6,7]])
b= a.copy()
a.fill(1)
print(a)
print(b)


== np.zeros(n_embed, 'f') in build_fillvalue function file name is word_embedding_features.py

'f' , 'c' - it's order

d = numpy.zeros(3) not adding dtype
  array([0., 0., 0.])

d = numpy.zeros(3,'c') - its order in row wise
  array([b'', b'', b''], dtype='|S1')

d = numpy.zeros(3,'f') its order in columns wise
  array([0., 0., 0.], dtype=float32)


***************************print('Build word extra features...')


input is ::: WordExtraFeaturizer(config, vocab)

1. vocab :: Build vocabulary 
2. config parameter

return output is :::: word_extra_featurizer
1.cofig 2.default-config 3.extra_default_config 4.vocab 5.featurizers 6. registry


 input ::: word_extra_featurizer(vocab)
output :::  return output is ::: we have 1 ndarray (47, 0) size




aa= np.array([[1,2,3,4],[11,12,13,14]])
aa.shape  = (2,4) (row, columns)
axis = 0 - row
axis =1 - columns


aa=np.empty([5,0])
np.concatenate([aa,*[]], axis=1)


***************************print('Build models...')


based on - config.cv parameter we are need at least 2 word features cv so baiscally we are add random noise in data logic inside of build_word_features function 


input ::: build_word_features
1. word_embedding_featurizer 2. embedding_matrices 3. word_extra_features

output :::  return output is ::: we have 2 - ndarray as per config-cv parameter size is (47, 300) 




#build_word_features method ::

import numpy as np

a = np.array([[1,2,3,4,5],[6,7,8,9,10]])
b = np.array([[11,12,13,14,15],[16,17,18,19,20]])

embedding_matrices = {}
embedding_matrices.update({'word2vec':a})
embedding_matrices.update({'pretrained':b})

embedding_matrices.values()

list(embedding_matrices.values())

embedding = np.stack(list(embedding_matrices.values()))

ouput::
array([[[ 1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10]],

       [[11, 12, 13, 14, 15],
        [16, 17, 18, 19, 20]]])


shape = (2,2,5)


embedding[0]:::

array([[ 1,  2,  3,  4,  5],
       [ 6,  7,  8,  9, 10]])

(embedding[0]==0):::

array([[False, False, False, False, False],
       [False, False, False, False, False]])


(embedding[0]==0).all(axis=1) ::

select value in columns wise ->::::   array([False, False])

unk = (embedding[0]==0).all(axis=1)


embedding[0, ~unk].mean() ::::

embedding[0] :::

array([[ 1,  2,  3,  4,  5],
       [ 6,  7,  8,  9, 10]])

embedding[0,[0]]:::
array([[1, 2, 3, 4, 5]])

embedding[0,[0,1]]:::

array([[ 1,  2,  3,  4,  5],
       [ 6,  7,  8,  9, 10]])

 embedding[0,~unk]:::

 array([[ 1,  2,  3,  4,  5],
       [ 6,  7,  8,  9, 10]])


embedding[0,~unk].mean()::: we can get some value same in with std method value

import numpy as np

a = np.array([True, False, True])
b = np.array([True, False, True])

c = a & b
array([True, False,  True])

c.sum()

noise = np.random.normal(-0.555, 0.35, (2, 5))
output :::
array([[-0.18526755, -0.56126516, -0.97014321,  0.13139335, -0.45458216],
       [-0.23502603, -0.71499231, -0.86441308, -0.48076677,  0.2464266 ]])



******************build_model()


input ::: build_model method 

1. as per cv we have 2 ndarray we can pass one by one in form of word_features 2. config 3. sentence_extra_featurizer.n_dims

output :::  return output is ::: we have 2 - as per config-cv parameter we have 2 binarray classifiers with structure of model 

BinaryClassifier(
  (embedding): Embedding(
    (module): Embedding(47, 300)
    (dropout1d): Dropout(p=0.2, inplace=False)
  )
  (encoder): Encoder(
    (module): LSTMEncoder(
      (rnns): ModuleList(
        (0): LSTM(300, 128, batch_first=True, bidirectional=True)
        (1): LSTM(256, 128, batch_first=True, bidirectional=True)
      )
    )
  )
  (aggregator): Aggregator(
    (module): MaxPoolingAggregator()
  )
  (mlp): MLP(
    (layers): Sequential(
      (0): Linear(in_features=256, out_features=128, bias=True)
      (1): ReLU(inplace=True)
      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU(inplace=True)
      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (out): Linear(in_features=128, out_features=1, bias=True)
  (lossfunc): BCEWithLogitsLoss()
)



build model have steps before move into starting tranining to machine learning model ::

1. embedding 2. encoder 3. Aggregator 4. MLP (nlp move to machine language processing ) 
5. ouput (nn.linear) 6. lossfunc

need steps with debug points pass so basically clear understanding .




***************************print('Start training...')

splitter :: its return 2 splits becuase of config.cv =2 - StratifiedKFold 

splitter.split ::  iterater both value train and target value ::: train-dataset.df  and train_dataset.df.target

train_indices getting from splitter.split method basically train_indices pass into build_labeled_dataset method 

train_tensor = train_dataset.build_labeled_dataset(train_indices) ::  build_labeled_dataset method return for value 

1. tokens's  tokens-ids

train_iter = DataLoader(train_tensor, sampler=sampler, batch_size=batchsize)
valid_iter = DataLoader(valid_tensor, batch_size=config.batchsize_valid)


-- dataloader object contains train_tensor as "datatset" attribute also others attributes are there .

1. tokens | tokens-ids 2. tokens | tokesns-ids 3. target 4. weights 

then iter using forloop --- train_iter variable and inside forloop we call model.train() and optimizaers and calculate loss and outout as per batch wise .

loss, output = model.calc_loss(*batch)

X = (token_ids, max_len) , (16, 72)


predict_features(self, X, X2):  (basically used in :: numpy)

example ::::

import numpy as np

X = np.array([[1,2,3,4,5],[6,7,8,9,10], [0,0,0,0,0], [1,1,0,0,0], [0,0,0,0,0]])

mask = X != 0

output :  array([[ True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True],
       [False, False, False, False, False],
       [ True,  True, False, False, False],
       [False, False, False, False, False]])



maxlen = (mask == 1).any(axis=0).sum()

output ::5

X = X[:, :maxlen]

output ::: array([[ 1,  2,  3,  4,  5],
       [ 6,  7,  8,  9, 10],
       [ 0,  0,  0,  0,  0],
       [ 1,  1,  0,  0,  0],
       [ 0,  0,  0,  0,  0]])

mask = mask[:, :maxlen]

output:: array([[ True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True],
       [False, False, False, False, False],
       [ True,  True, False, False, False],
       [False, False, False, False, False]])


h = self.embedding(X) 

X is basically its tensor value ..while pass tensor into embedding forward then we can getting h variable (hidden weights)




======example with torch


import torch
X = torch.tensor([[1,2,3,4,5],[6,7,8,9,10], [0,0,0,0,0], [1,1,0,0,0], [0,0,0,0,0]])

mask = X != 0

maxlen = (mask == 1).any(dim=0).sum()
output:: tensor(5)

X = X[:, :maxlen]

mask = mask[:, :maxlen]


masked-attention ::: http://juditacs.github.io/2018/12/27/masked-attention.html


X
tensor([[ 1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10],
        [ 0,  0,  0,  0,  0],
        [ 1,  1,  0,  0,  0],
        [ 0,  0,  0,  0,  0]])


h = hs.max(dim=1)[0] 






==========================================================================


question ::: some basic understanding ... how to call WordbasedPreprocessor class using Preprocessor obj ?

ans :: Preprocessor -> PreprocessorPresets -> WordbasedPreprocessor (class inhertance)


question :: baisc flow of program

ans :: models -> config -> presets -> 1. config
					                           2. preprocessing -> registry
					                           3. modules 



=================

1. quora_02_12.py 

2. load v1_8_1_bilstm_w2v_rnd.py using importlib lib ...it just template of all class 

3. then call --- class ExperimentConfigBuilder (build method) from config file
3.1 self.default_config is set while load file v1_8_1_bilstm_w2v_rnd.py 
3.2 cls.default_config and cls.registry is class method attributes .

4. build method call each modules of "add_args" module method 

5. build method call each modules of cls.default_extra_config is set while load file v1_8_1_bilstm_w2v_rnd.py via calling add_extra_args . (similar calling pattern add_args)


path set:: -m models / baseline / v1-8-1-bilstm-w2v-rnd.py (pre-loading part)

1. config-builder base arguments 
2. load presets-v1-8-1-bilstm-w2v-rnd.py 
	1. load config  2. load preprocessing modules 
	-> text normalizer , text-tokenlizer wrapper , word embedding feature, word extra feature wrapper , sentence extra feature wrapper, 


=========================================================================================










